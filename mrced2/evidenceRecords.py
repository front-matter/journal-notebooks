#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Mar 17 14:53:14 2021

@author: martynrittman
"""

import requests
import json
from urllib.parse import urlparse


class evidenceRecords:
    ''' Query and perform operations on activity logs from Crossref Event Data '''

    def __init__(self):

        self.outputFile = 'test.json'

        self.jsonData = []
        self.urlPrefix = 'https://evidence.eventdata.crossref.org/evidence/'
        self.query = ''

    # =====================================

    # API querying

    def buildQuery(self, url):
        '''
        Construct a query for an activity log, prints the query.

        Parameters
        ----------
        url: str
            string containing the suffix of an activity log URL, or the full URL

        Returns
        -------
        None



        '''

        # prefix with the correct url if necessary
        try:
            x = url[:49] == self.urlPrefix
        except IndexError:
            x = False
        if not(x):
            url = self.urlPrefix + url

        self.query = url

        print(self.query)

    def runQuery(self, quiet=False):
        '''
        Runs the query generated by buildQuery. self.success is True if the query
        runs successfully.

        Parameters
        ----------
        quiet : boolean
            There is no printed output if True. The default is False.
        saveToFile : boolean
            Saves the json contents of the activity log to self.outputFile if 
            set to True. The default is True.

        Returns
        -------
        None.

        '''

        r = requests.get(self.query)

        # stop if there wasn't a response
        if r.status_code in (200, 201):
            self.success = True
            # find and save the next cursor (add to next call to iterate results pages)
            self.jsonData = r.json()

            if saveToFile:
                with open(self.outputFile, 'w') as f:
                    # save the json result to file
                    json.dump(self.jsonData, f)
                    if not(quiet):
                        print("output file written to " + self.outputFile)

        else:
            self.success = False

    # =====================================

    # Analysis functions

    def filterBySubject(self, subjFilter):
        '''

        Filter the subject of each action (in each page) by some text. The subject
        has various fields, and the input needs to specify the field (dict key)
        and the search text (dict value).

        Parameters
        ----------
        subjFilter: dict
            fields and values to filter for in subject

        Returns
        -------
        list of dicts: 
            filtered observations

        '''

        # do the filtering

        filteredPages = []

        # run through pages (probably only one?)
        for page in self.jsonData['pages']:
            filteredActions = []  # output for actions we want to keep
            # run through actions
            for action in page['actions']:

                for field in subjFilter:
                    try:
                        # search using substrings
                        if subjFilter[field] in action['subj'][field]:
                            filteredActions.append(action)

                            # searching is OR-type: it's a match if we find anything
                            break

                    except KeyError:
                        pass

            # add stuff we found on this page to the output
            filteredPages.append({'actions': filteredActions})

        # create an activitylogs object
        filteredLog = evidenceRecords()
        filteredLog.success = 1

        # add the full json record
        filteredLog.jsonData = self.jsonData.copy()
        filteredLog.jsonData['pages'] = filteredPages

        return filteredLog

    def countDomains(self):
        '''
        Get the number of domains used in observations, along with counts of 
        observations and events. Don't use this for Twitter, see countTwitterDomains.

        Returns
        -------
        Domains: dict
            Dictionary with domains as keys and number of observations and events as entries.
            Also calculates the ratio of events to observations.

        '''

        domains = {}

        # get to the list of actions
        for page in self.jsonData['pages']:
            for action in page['actions']:

                # get the candidate url from the action
                try:
                    url = action['subj']['url']
                except KeyError:
                    continue

                # pull the domain from the url
                urlp = urlparse(url)
                domain = urlp.netloc

                # add the domain to the output if needed
                if not (domain in domains):
                    domains[domain] = {'observations': 0, 'events': 0}

                # record observations and events
                domains[domain]['observations'] += len(
                    action['processed-observations'])
                domains[domain]['events'] += len(action['events'])

        # add success rates

        for domain in domains:
            domains[domain]['event-ratio'] = domains[domain]['events'] / \
                float(domains[domain]['observations'])

        return domains

    def countTwitterDomains(self):
        '''
        Get the number of domains used in observations, along with counts of observations and events

        Parameters
        ----------
        source: str
            The source, if known. Default is wikipedia. Different sources have the URL in 
            different parts of the observations.


        Returns
        -------
        Domains: dict
            Dictionary with domains as keys and number of observations and events as entries.
            Also calculates the ratio of events to observations.

        '''

        domains = {}

        try:
            if not(self.jsonData['agent']['name'] == 'twitter-agent'):
                return domains
        except:
            # something went wrong
            "couldn't locate agent name"
            return domains

        # get to the lists of candidates and events
        for page in self.jsonData['pages']:
            for action in page['actions']:
                for obs in action['processed-observations']:
                    try:
                        for candidate in obs['candidates']:
                            try:
                                url = candidate['value']
                            except KeyError:
                                continue

                            # add candidate url to output
                            urlp = urlparse(url)
                            domain = urlp.netloc

                            if not(domain in domains):
                                domains[domain] = {
                                    'observations': 0, 'events': 0}

                            domains[domain]['observations'] += 1
                    except:
                        pass

                for event in action['events']:

                    try:
                        url = event['obj']['url']
                    except KeyError:
                        continue

                    # add candidate url to output
                    urlp = urlparse(url)
                    domain = urlp.netloc

                    if not(domain in domains):
                        domains[domain] = {'observations': 0, 'events': 0}

                    domains[domain]['events'] += 1

        # add success rates
        for domain in domains:
            try:
                domains[domain]['event-ratio'] = domains[domain]['events'] / \
                    float(domains[domain]['observations'])
            except:
                # catch division by zero case
                domains[domain]['event-ratio'] = 0

        return domains


if __name__ == '__main__':

    import pprint

    al = evidenceRecords()

    al.buildQuery('20201201-twitter-ec7c88d7-89d0-49cd-b940-f8b10bdedb1c')

    al.runQuery()

    fl = {'pid': 'de.wikipedia.org'}
    newal = al.filterBySubject(fl)
    domains = al.countTwitterDomains()

    pprint.pprint(domains)
